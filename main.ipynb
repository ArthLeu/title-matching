{"metadata":{"kernelspec":{"name":"python388jvsc74a57bd006f0b86e6bdba99e28d98577ff6ec36020717253ecd624b165753310a3730b6d","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import glob\n","from collections import defaultdict\n","\n","from functools import partial\n","import random\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","\n","import nltk\n","import string\n","\n","# Spacy model\n","import spacy\n","from spacy.util import minibatch, compounding\n","#from spacy.training.example import Example # version 3 only\n","\n","KAGGLE = True"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":1,"outputs":[]},{"source":["## Preprocessing"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["if KAGGLE:\n","    from model.model import longest_consecutive_caps as LCC\n","    from model.model import KMP\n","\n","    train_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\n","    sample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n","    train_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\n","    test_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\"\n","\n","else:\n","    from model import longest_consecutive_caps as LCC\n","    from model import KMP\n","    \n","    train_df = pd.read_csv(\"dataset/train.csv\")\n","    sample_sub = pd.read_csv('dataset/sample_submission.csv')\n","    train_fp = \"dataset/train/\"\n","    test_fp = \"dataset/test/\""],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#!python -m spacy download en_core_web_trf"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#train_df.head(5)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.030078Z","iopub.execute_input":"2021-06-02T08:59:11.030546Z","iopub.status.idle":"2021-06-02T08:59:11.034559Z","shell.execute_reply.started":"2021-06-02T08:59:11.030508Z","shell.execute_reply":"2021-06-02T08:59:11.033393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#train_df.info()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.038313Z","iopub.execute_input":"2021-06-02T08:59:11.038765Z","iopub.status.idle":"2021-06-02T08:59:11.045212Z","shell.execute_reply.started":"2021-06-02T08:59:11.038727Z","shell.execute_reply":"2021-06-02T08:59:11.044344Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def read_append_return(filename, train_files_path=train_fp, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \n","train_df['text'] = train_df['Id'].progress_apply(read_append_return)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 19661/19661 [00:05<00:00, 3877.35it/s]CPU times: user 4.19 s, sys: 900 ms, total: 5.09 s\n","Wall time: 5.08 s\n","\n"]}]},{"cell_type":"code","source":["def text_cleaning(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    #text = re.sub(' +', ' ', str(text).lower()).strip()\n","    text = re.sub('[^A-Za-z0-9 ]+', '|', str(text).lower())\n","    #text = ''.join([k for k in text if k not in string.punctuation])\n","    #text = re.sub('[^A-Za-z0-9.]+', ' ', str(text).lower()).strip()\n","#     text = re.sub(\"/'+/g\", ' ', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","train_df['text'] = train_df['text'].progress_apply(text_cleaning)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 19661/19661 [00:36<00:00, 539.98it/s]CPU times: user 35.6 s, sys: 1e+03 ms, total: 36.6 s\n","Wall time: 36.4 s\n","\n"]}]},{"cell_type":"code","source":["a = train_df[\"Id\"].nunique()\n","b = train_df[\"Id\"].size\n","print(a, b)\n","print(a/b)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["14316 19661\n0.7281420070189716\n"]}]},{"cell_type":"code","source":["#train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"source":["## Training\n","### Choose spaCy model and prepare training data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# training configurations\n","output_dir=\"output/\"\n","n_iter = 25 # number of training iteration\n","\n","# select model backbone\n","model = None \n","#model = \"specified\" # specified for transformer + ner only\n","#model = \"generic\" # a model based on existing en_core_web_sm\n","\n","token_anno = \"entities\" # note this may require changes for v3.0+ transformer models"]},{"cell_type":"code","source":["TRAIN_DATA = []\n","\n","for index, row in tqdm(train_df.iterrows()):\n","    # get text of each sample test\n","    train_text = row['text']\n","    row_id = row['Id']\n","    label = row['cleaned_label']\n","    m = len(label)\n","    text = train_text.lower().split('|')\n","    #text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin matching\n","    for sentence in text:\n","        # clean text \n","        #sentence = clean_text(sentence)\n","        indexed = KMP(label, sentence)\n","        if indexed != []:\n","            n = len(sentence)\n","            elist = []\n","            for i in indexed:\n","                end = m + i\n","                a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n","                b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n","                if a and b:\n","                    entity = (i, end, \"DATASET\") \n","                    elist.append(entity)\n","            if elist == []: break # TEST (REMOVE IF NEEDED)\n","            x = (sentence, {token_anno:elist})\n","            TRAIN_DATA.append(x)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["19661it [07:14, 45.24it/s]\n"]}]},{"source":["### Configure training pipeline and train the spaCy model\n","\n","Download official model with `python3 -m spacy download en_core_web_sm`"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU: True\n","Loaded generic model en_core_web_sm\n"]}],"source":["print(\"Using GPU:\", spacy.prefer_gpu())\n","\n","# load the model\n","if model == \"specified\":\n","    nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","elif model == \"generic\":\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    print(\"Loaded generic model en_core_web_sm\")\n","elif model is not None:\n","    nlp = spacy.load(model)  \n","    print(\"Loaded model '%s'\" % model)\n","else:\n","    nlp = spacy.blank('en')  \n","    print(\"Created blank 'en' model\")\n","\n","# add ner component to pipeline\n","if 'ner' not in nlp.pipe_names:\n","    ner = nlp.create_pipe('ner') # version < 3.0\n","    nlp.add_pipe(ner, last=True) # verions < 3.0\n","    #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n","    #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\n","else:\n","    ner = nlp.get_pipe('ner')\n","\n","# add all labels to the ner\n","for _, annotations in TRAIN_DATA:\n","    for ent in annotations.get('entities'):\n","        ner.add_label(ent[2])\n","#ner.add_label(\"DATASET\") # only one category so no for loop\n","\n","# configure optimizer (may not work with spaCy v3 transformers)\n","optimizer = nlp.begin_training()\n","\n","# configure pipeline components to disable\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"]},{"cell_type":"code","source":["# start training\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","    for itn in range(n_iter):\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001)) # HYPERPARAMETERS\n","        # version < 3.0 use thi\n","        for batch in tqdm(batches):\n","            texts, annotations = zip(*batch)\n","            nlp.update(\n","                texts, # or [texts] if not using batch\n","                annotations, # or [annotations] if not using batch\n","                drop=0.5,  \n","                sgd=optimizer,\n","                losses=losses)\n","        \n","        # For spaCy v3.0+ use the similar for loops in archived.ipynb\n","            \n","        print(\"\\titeration %d, ner loss: %.2f\"%(itn+1, losses['ner']))"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["2882it [01:56, 24.64it/s]\n","4it [00:00, 28.96it/s]\titeration 1, ner loss: 325564.31\n","2882it [01:53, 25.29it/s]\n","4it [00:00, 33.17it/s]\titeration 2, ner loss: 311908.03\n","2882it [01:54, 25.22it/s]\n","3it [00:00, 25.05it/s]\titeration 3, ner loss: 309088.46\n","2882it [01:54, 25.21it/s]\n","4it [00:00, 32.83it/s]\titeration 4, ner loss: 308337.13\n","2882it [01:55, 25.04it/s]\n","3it [00:00, 25.71it/s]\titeration 5, ner loss: 307145.44\n","2882it [01:57, 24.51it/s]\n","3it [00:00, 28.95it/s]\titeration 6, ner loss: 306298.50\n","2882it [01:57, 24.48it/s]\n","3it [00:00, 28.49it/s]\titeration 7, ner loss: 306292.75\n","2882it [01:52, 25.53it/s]\n","4it [00:00, 31.77it/s]\titeration 8, ner loss: 305150.08\n","2882it [01:50, 26.04it/s]\n","4it [00:00, 32.66it/s]\titeration 9, ner loss: 305558.32\n","2882it [01:49, 26.21it/s]\n","4it [00:00, 30.91it/s]\titeration 10, ner loss: 305292.76\n","2882it [01:46, 26.98it/s]\n","4it [00:00, 32.66it/s]\titeration 11, ner loss: 304080.17\n","2882it [01:40, 28.63it/s]\n","4it [00:00, 35.42it/s]\titeration 12, ner loss: 304364.03\n","2882it [01:40, 28.68it/s]\n","4it [00:00, 34.62it/s]\titeration 13, ner loss: 304513.57\n","2882it [01:40, 28.69it/s]\n","4it [00:00, 34.77it/s]\titeration 14, ner loss: 304190.06\n","2882it [01:47, 26.89it/s]\n","4it [00:00, 32.24it/s]\titeration 15, ner loss: 304178.49\n","2882it [01:51, 25.96it/s]\n","4it [00:00, 33.00it/s]\titeration 16, ner loss: 303957.08\n","2882it [01:50, 26.14it/s]\n","4it [00:00, 30.54it/s]\titeration 17, ner loss: 303461.12\n","2882it [01:49, 26.36it/s]\n","4it [00:00, 30.87it/s]\titeration 18, ner loss: 304096.98\n","2882it [01:48, 26.58it/s]\n","4it [00:00, 32.60it/s]\titeration 19, ner loss: 303565.97\n","2882it [01:48, 26.44it/s]\n","4it [00:00, 30.80it/s]\titeration 20, ner loss: 303739.23\n","2882it [01:50, 26.11it/s]\n","3it [00:00, 27.40it/s]\titeration 21, ner loss: 303307.49\n","2882it [01:52, 25.59it/s]\n","4it [00:00, 30.82it/s]\titeration 22, ner loss: 303730.14\n","2882it [01:44, 27.46it/s]\n","4it [00:00, 31.87it/s]\titeration 23, ner loss: 303152.90\n","2882it [01:51, 25.95it/s]\n","3it [00:00, 27.84it/s]\titeration 24, ner loss: 302855.11\n","2882it [01:56, 24.75it/s]\titeration 25, ner loss: 303186.38\n","\n"]}]},{"source":["## Testing and Exporting Model"],"cell_type":"markdown","metadata":{}},{"source":["# sample test\n","for text, _ in TRAIN_DATA:\n","    doc = nlp(text)\n","    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n","    break\n","\n","# save model\n","if output_dir is not None:\n","    output_dir = Path(output_dir)\n","    if not output_dir.exists():\n","        output_dir.mkdir()\n","    nlp.to_disk(output_dir)\n","    print(\"Saved model to\", output_dir)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved model to output\n"]}]},{"source":["## Inference with Trained Model\n","Generates the submission file"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def clean_text(txt):\n","    ''' DO NOT DELETE: Official function for submission text cleaning '''\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n","    #return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 4/4 [00:00<00:00, 1164.52it/s]CPU times: user 4.64 ms, sys: 3.99 ms, total: 8.63 ms\n","Wall time: 7.43 ms\n","\n"]}]},{"cell_type":"code","source":["# STRING MATCHING BLOCK\n","temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n","temp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\n","temp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n","\n","existing_labels = set(temp_1 + temp_2 + temp_3)\n","id_list = []\n","lables_list = []\n","\n","# load model 'EN'\n","#nlp2 = spacy.load(output_dir) # loading an model can be slower?\n","\n","for index, row in tqdm(sample_sub.iterrows()):\n","    # get text of each sample test\n","    sample_text = row['text']\n","    row_id = row['Id']\n","    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n","\n","    cleaned_labels = temp_df['cleaned_label'].to_list()\n","\n","    texts = sample_text.lower().split('.')\n","    #text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin search\n","    # matching\n","    \n","    for known_label in existing_labels:   # for each label in the known set\n","        # EXACT MATCH\n","        if known_label in sample_text.lower():   # find the EXACT label in text \n","            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n","    \n","    # THIS METHOD BELOW IS MUCH FASTER!!!!!\n","    # SOURCE: https://spacy.io/usage/processing-pipelines\n","    # SOURCE 2: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n","    # THANK YOU SOOO MUCH!!!\n","    # Disabling pipeline components also helps\n","    for doc in nlp.pipe(texts, disable=other_pipes):\n","        cleaned_labels.extend([clean_text(entity.text) for entity in doc.ents if entity.label_ == \"DATASET\"])\n","    \n","\n","    #for sentence in texts:   \n","        #doc = nlp(text_cleaning(sentence))\n","    #    doc = nlp2(text_cleaning(sentence))\n","\n","    #    for entity in doc.ents:\n","    #        if entity.label_ == 'DATASET':\n","    #            cleaned_labels.append(clean_text(entity.text))   \n","            \n","        # CASE 1: FUZZYMATCH (see archived)\n","\n","        # CASE 2: Consecutive Capitalizations (see archived)\n","        \n","        \n","    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n","    cleaned_labels = set(cleaned_labels)\n","    lables_list.append('|'.join(cleaned_labels))\n","    id_list.append(row_id)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["4it [00:01,  2.80it/s]\n"]}]},{"cell_type":"code","source":["submission = pd.DataFrame()\n","submission['Id'] = id_list\n","submission['PredictionString'] = lables_list"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","submission.head()"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     Id  \\\n","0  2100032a-7c33-4bff-97ef-690822c43466   \n","1  2f392438-e215-4169-bebf-21ac4ff253e1   \n","2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n","3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n","\n","                                    PredictionString  \n","0  106k|5 000|alzheimer s disease neuroimaging in...  \n","1  higher education figure|nces common core of da...  \n","2  slosh point|slosh model|coastal management coo...  \n","3  rural urban continuum codes|program participan...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>106k|5 000|alzheimer s disease neuroimaging in...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>higher education figure|nces common core of da...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>slosh point|slosh model|coastal management coo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>rural urban continuum codes|program participan...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["for item in submission[\"PredictionString\"]:\n","    print(item)\n","    print()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["106k|5 000|alzheimer s disease neuroimaging initiative adni|adni|alzheimer s disease neuroimaging initiative adni \n\nhigher education figure|nces common core of data|education e|higher education 6 upper secondary education|mathematics teacher and the science teacher|world arguments|early childhood education|common core of data|program for international student|education qualifications|higher education|behavioral sciences|program taken in higher education departments|national education systems|high school|education in france|education science|school teacher salaries|higher education level|high school for upper secondary school|trends in international mathematics and science study|education figure|education in canada|program focusing on research and taken|beginning of the school year|education 6|trends in teacher preparation|higher education had higher|education and\n\nslosh point|slosh model|coastal management coordinates|coastal observation station|slosh grid|coastal change science along our|education and outreach is a critical component of|coastal change hazards portal and slr|slosh display|slosh grids|north of buxton|slosh mom|coastal management coastal management hottopics sea level rise|slosh storm|slosh inundation|sea lake and overland surges from hurricanes|national oceanic atmospheric administration noaa|slosh and|high tide|coastal management has|slosh output|slosh meows|slosh moms|slosh safir|coastal ocean waters|coastal change hazards|higher high water|high tide in order to identify the development|of change figure|coastal erosion study|noaa storm surge inundation\n\nrural urban continuum codes|program participant respondents are equally likely to purchase whole wheat bread as non program|high school graduate|urban continuum codes|program and non program|program participants and non participants\n\n"]}]}]}