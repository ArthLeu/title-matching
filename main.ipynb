{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport glob\nfrom collections import defaultdict\nfrom functools import partial\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n#from tqdm.autonotebook import tqdm\n\nimport nltk\nimport string\nfrom fuzzywuzzy import fuzz\n\n#from model import longest_consecutive_caps as LCC\n#from model import KMP\nfrom model.model import longest_consecutive_caps as LCC\nfrom model.model import KMP\n\n\n# Spacy model\nimport spacy\n#from __future__ import unicode_literals, print_function\n#import plac\nimport random\nfrom pathlib import Path\nfrom tqdm import tqdm\n#from spacy.training.example import Example # v3 only\n\n\noutput_dir=\"output/\"\nmodel = None \n#model = \"specified\" # specified for transformer + ner only\nn_iter = 10 # number of training iteration","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python -m spacy download en_core_web_trf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df = pd.read_csv(\"dataset/train.csv\")\n#sample_sub = pd.read_csv('dataset/sample_submission.csv')\n#train_fp = \"dataset/train/\"\n#test_fp = \"dataset/test/\"\n\ntrain_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\nsample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\ntrain_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\ntest_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\"","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.030078Z","iopub.execute_input":"2021-06-02T08:59:11.030546Z","iopub.status.idle":"2021-06-02T08:59:11.034559Z","shell.execute_reply.started":"2021-06-02T08:59:11.030508Z","shell.execute_reply":"2021-06-02T08:59:11.033393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.038313Z","iopub.execute_input":"2021-06-02T08:59:11.038765Z","iopub.status.idle":"2021-06-02T08:59:11.045212Z","shell.execute_reply.started":"2021-06-02T08:59:11.038727Z","shell.execute_reply":"2021-06-02T08:59:11.044344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_append_return(filename, train_files_path=train_fp, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = re.sub(' +', ' ', str(text).lower()).strip()\n    #text = ''.join([k for k in text if k not in string.punctuation])\n    #text = re.sub('[^A-Za-z0-9\\(\\)]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"/'+/g\", ' ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train_df[\"Id\"].nunique()\nb = train_df[\"Id\"].size\nprint(a, b)\nprint(a/b)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nfrom model import search_sentences\nfrom custom_classes import PosMap\n```","metadata":{}},{"cell_type":"markdown","source":"```\n%%time\ntqdm.pandas()\n\ni = 0\nlimit = 100\np = 5\nq = 4\npre_tf = PosMap(p)\npost_tf = PosMap(q)\n\nfor row in tqdm(train_df.iterrows()):\n    label = row[1][\"cleaned_label\"]\n    text = row[1][\"text\"].lower()\n    act, deact = search_sentences(label, text, pre=p, post=q)\n\n    for j in range(len(act)):\n        pre_words = act[j].split()[::-1]\n        post_words = deact[j].split()\n        for k in range(min([len(pre_words), p])):\n            try:\n                word = pre_words[k]\n                pre_tf[k][word] += 1\n            except IndexError:\n                print(act[j])\n        \n        for k in range(min([len(post_words), q])):\n            try:\n                word = post_words[k]\n                post_tf[k][word] += 1\n            except IndexError:\n                print(deact[j])\n\n    i += 1\n    #if i >= limit: break\n```","metadata":{"tags":[]}},{"cell_type":"markdown","source":"```\npre_tf.plot(idx=0) # the first closest pre words\npre_tf.plot(idx=1) # the second closest pre words\npre_tf.plot(idx=2) # the third\npre_tf.plot(idx=3)\n```","metadata":{}},{"cell_type":"markdown","source":"```\npost_tf.plot(idx=0)\npost_tf.plot(idx=1)\npost_tf.plot(idx=2)\n```","metadata":{}},{"cell_type":"code","source":"#train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    ''' DO NOT DELETE: Official function for submission text cleaning '''\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n    #return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train data prep\nTRAIN_DATA = []\nif model == \"specified\":\n    token_anno = \"entities\"\nelse:\n    token_anno = \"entities\"\n\n\nfor index, row in tqdm(train_df.iterrows()):\n    # get text of each sample test\n    train_text = row['text']\n    row_id = row['Id']\n    label = row['cleaned_label']\n    m = len(label)\n    #text = sample_text.lower().split('.')\n    text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n\n    # begin matching\n    for sentence in text:\n        # clean text \n        sentence = clean_text(sentence)\n        indexed = KMP(label, sentence)\n        if indexed != []:\n            n = len(sentence)\n            elist = []\n            for i in indexed:\n                end = m+i\n                a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n                b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n                if a and b:\n                    entity = (i, end, \"DATASET\") \n                    elist.append(entity)\n            if elist == []: break # TEST (REMOVE IF NEEDED)\n            x = (sentence, {token_anno:elist})\n            TRAIN_DATA.append(x)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy.util import minibatch, compounding\n#load the model\nn_iter = 15 # number of training iteration (overriding n_iter declared in cell 1)\nprint(spacy.prefer_gpu())\n\nif model == \"specified\":\n    nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\nelif model == \"generic\":\n    nlp = spacy.load(\"en_core_web_sm\")\nelif model is not None:\n    nlp = spacy.load(model)  \n    print(\"Loaded model '%s'\" % model)\nelse:\n    nlp = spacy.blank('en')  \n    print(\"Created blank 'en' model\")\n\n\n#set up the pipeline\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.create_pipe('ner') # version < 3.0\n    nlp.add_pipe(ner, last=True) # verions < 3.0\n    #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n    #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\nelse:\n    ner = nlp.get_pipe('ner')\n\n#for _, annotations in TRAIN_DATA:\n#    for ent in annotations.get('entities'):\n#        ner.add_label(ent[2])\nner.add_label(\"DATASET\") # only one category so no for loop\n\n\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    if model == None: \n        optimizer = nlp.begin_training()\n    for itn in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        batches = minibatch(TRAIN_DATA, size=compounding(4., 16., 1.001)) # HYPERPARAMETERS\n        for batch in tqdm(batches):\n            texts, annotations = zip(*batch)\n        #for texts, annotations in tqdm(TRAIN_DATA):\n            \n            #doc = nlp.make_doc(texts) # version >= 3\n            #example = Example.from_dict(doc, annotations) # version >= 3\n            #nlp.update(\n            #    [example],\n            #    drop=0.5, \n            #    sgd=optimizer,\n            #    losses=losses) # version >= 3\n            nlp.update(\n                texts, # or [texts] if not using batch\n                annotations, # or [annotations] if not using batch\n                drop=0.5,  \n                sgd=optimizer,\n                losses=losses)\n        print(\"iteration\", itn+1,\"ner loss\", round(losses['ner'], 3))","metadata":{"tags":["outputPrepend"],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample test\nfor text, _ in TRAIN_DATA:\n    doc = nlp(text)\n    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n    break\n\n# save model\nif output_dir is not None:\n    output_dir = Path(output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir()\n    nlp.to_disk(output_dir)\n    print(\"Saved model to\", output_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# STRING MATCHING BLOCK\ntemp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\n# load model 'EN'\nnlp2 = spacy.load(output_dir)\nfor index, row in tqdm(sample_sub.iterrows()):\n    # get text of each sample test\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n\n    #text = sample_text.lower().split('.')\n    text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n\n    # begin search\n    # matching\n    \n    for known_label in existing_labels:   # for each label in the known set\n        # EXACT MATCH\n        if known_label in sample_text.lower():   # find the EXACT label in text \n            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n            \n    for sentence in text:\n        doc = nlp2(clean_text(sentence))\n\n        for entity in doc.ents:\n            if entity.label_ == 'DATASET':\n                cleaned_labels.append(clean_text(entity.text))   \n            \n        # CASE 1: FUZZY MATCH\n        #value = fuzz.partial_ratio(sentence.lower(), known_label) # I moved .lower() here\n        #if value > 85 and value < 100:\n            # print('value: ', str(value), known_label) # Alex, you might wanna see what this prints\n            # cleaned_labels.append(clean_text(known_label))\n    \n        # CASE 2: for unknown labels\n        # sentence filtering (Longest Consecutive Capitalization)\n        #print(sentence)\n#             length, rate, filtered_sentence = LCC(sentence)\n#             if rate <= 0 or length == 0 or (length == 1 and not sentence.isupper()): \n#                 continue # no consecutive caps found\n#             # <insert classifier here>\n#             else:\n#                 for keyword in [\"dataset\", \"data\", \"database\", \"survey\", \"study\", \"research\", \"statistics\"]:\n#                     if keyword in filtered_sentence.lower():\n#                         #pass\n#                         cleaned_labels.append(clean_text(filtered_sentence)) # naive\n        \n    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nsubmission.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in submission[\"PredictionString\"]:\n    print(item)\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":null,"outputs":[]}]}