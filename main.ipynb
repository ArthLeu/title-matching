{"metadata":{"kernelspec":{"name":"python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1","display_name":"Python 3.8.5 64-bit"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import glob\n","from collections import defaultdict\n","from functools import partial\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","#from tqdm.autonotebook import tqdm\n","\n","import nltk\n","import string\n","#from fuzzywuzzy import fuzz\n","\n","from model import longest_consecutive_caps as LCC\n","from model import KMP\n","#from model.model import longest_consecutive_caps as LCC\n","#from model.model import KMP\n","\n","\n","# Spacy model\n","import spacy\n","#from __future__ import unicode_literals, print_function\n","#import plac\n","import random\n","from pathlib import Path\n","from tqdm import tqdm\n","#from spacy.training.example import Example # version 3 only\n","\n","\n","output_dir=\"output/\"\n","model = None \n","#model = \"specified\" # specified for transformer + ner only\n","n_iter = 10 # number of training iteration"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!python -m spacy download en_core_web_trf"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(\"dataset/train.csv\")\n","sample_sub = pd.read_csv('dataset/sample_submission.csv')\n","train_fp = \"dataset/train/\"\n","test_fp = \"dataset/test/\"\n","\n","#train_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\n","#sample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n","#train_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\n","#test_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\""],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_df.head(5)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.030078Z","iopub.execute_input":"2021-06-02T08:59:11.030546Z","iopub.status.idle":"2021-06-02T08:59:11.034559Z","shell.execute_reply.started":"2021-06-02T08:59:11.030508Z","shell.execute_reply":"2021-06-02T08:59:11.033393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_df.info()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.038313Z","iopub.execute_input":"2021-06-02T08:59:11.038765Z","iopub.status.idle":"2021-06-02T08:59:11.045212Z","shell.execute_reply.started":"2021-06-02T08:59:11.038727Z","shell.execute_reply":"2021-06-02T08:59:11.044344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_append_return(filename, train_files_path=train_fp, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \n","train_df['text'] = train_df['Id'].progress_apply(read_append_return)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_cleaning(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    #text = re.sub(' +', ' ', str(text).lower()).strip()\n","    text = re.sub('[^A-Za-z0-9 ]+', '|', str(text).lower())\n","    #text = ''.join([k for k in text if k not in string.punctuation])\n","    #text = re.sub('[^A-Za-z0-9.]+', ' ', str(text).lower()).strip()\n","#     text = re.sub(\"/'+/g\", ' ', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","train_df['text'] = train_df['text'].progress_apply(text_cleaning)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = train_df[\"Id\"].nunique()\n","b = train_df[\"Id\"].size\n","print(a, b)\n","print(a/b)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"source":["### Prepare training data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["TRAIN_DATA = []\n","if model == \"specified\":\n","    token_anno = \"entities\"\n","else:\n","    token_anno = \"entities\"\n","\n","\n","for index, row in tqdm(train_df.iterrows()):\n","    # get text of each sample test\n","    train_text = row['text']\n","    row_id = row['Id']\n","    label = row['cleaned_label']\n","    m = len(label)\n","    text = train_text.lower().split('|')\n","    #text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin matching\n","    for sentence in text:\n","        # clean text \n","        #sentence = clean_text(sentence)\n","        indexed = KMP(label, sentence)\n","        if indexed != []:\n","            n = len(sentence)\n","            elist = []\n","            for i in indexed:\n","                end = m+i\n","                a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n","                b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n","                if a and b:\n","                    entity = (i, end, \"DATASET\") \n","                    elist.append(entity)\n","            if elist == []: break # TEST (REMOVE IF NEEDED)\n","            x = (sentence, {token_anno:elist})\n","            TRAIN_DATA.append(x)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"source":["### Train the spaCy model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["from spacy.util import minibatch, compounding\n","#load the model\n","n_iter = 25 # number of training iteration (overriding n_iter declared in cell 1)\n","print(spacy.prefer_gpu())\n","\n","if model == \"specified\":\n","    nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","elif model == \"generic\":\n","    nlp = spacy.load(\"en_core_web_sm\")\n","elif model is not None:\n","    nlp = spacy.load(model)  \n","    print(\"Loaded model '%s'\" % model)\n","else:\n","    nlp = spacy.blank('en')  \n","    print(\"Created blank 'en' model\")\n","\n","\n","#set up the pipeline\n","if 'ner' not in nlp.pipe_names:\n","    ner = nlp.create_pipe('ner') # version < 3.0\n","    nlp.add_pipe(ner, last=True) # verions < 3.0\n","    #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n","    #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\n","else:\n","    ner = nlp.get_pipe('ner')\n","\n","#for _, annotations in TRAIN_DATA:\n","#    for ent in annotations.get('entities'):\n","#        ner.add_label(ent[2])\n","ner.add_label(\"DATASET\") # only one category so no for loop\n","\n","\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","    if model == None: \n","        optimizer = nlp.begin_training()\n","    for itn in range(n_iter):\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001)) # HYPERPARAMETERS\n","        for batch in tqdm(batches):\n","            texts, annotations = zip(*batch)\n","            nlp.update(\n","                texts, # or [texts] if not using batch\n","                annotations, # or [annotations] if not using batch\n","                drop=0.5,  \n","                sgd=optimizer,\n","                losses=losses)\n","        #for batch in tqdm(batches):\n","        #for texts, annotations in tqdm(TRAIN_DATA):\n","        #    for texts, annotations in batch:\n","        #        doc = nlp.make_doc(texts) # version >= 3\n","        #        example = Example.from_dict(doc, annotations) # version >= 3\n","        #        nlp.update(\n","        #            [example],\n","        #            drop=0.5, \n","        #            sgd=optimizer,\n","        #            losses=losses) # version >= 3\n","            \n","        print(\"\\niteration\", itn+1,\"ner loss\", round(losses['ner'], 3))"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"source":["# sample test\n","#for text, _ in TRAIN_DATA:\n","#    doc = nlp(text)\n","#    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n","#    break\n","\n","# save model\n","if output_dir is not None:\n","    output_dir = Path(output_dir)\n","    if not output_dir.exists():\n","        output_dir.mkdir()\n","    nlp.to_disk(output_dir)\n","    print(\"Saved model to\", output_dir)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"source":["### Test the model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def clean_text(txt):\n","    ''' DO NOT DELETE: Official function for submission text cleaning '''\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n","    #return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STRING MATCHING BLOCK\n","temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n","temp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\n","temp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n","\n","existing_labels = set(temp_1 + temp_2 + temp_3)\n","id_list = []\n","lables_list = []\n","# load model 'EN'\n","#nlp2 = spacy.load(output_dir)\n","for index, row in tqdm(sample_sub.iterrows()):\n","    # get text of each sample test\n","    sample_text = row['text']\n","    row_id = row['Id']\n","    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n","\n","    cleaned_labels = temp_df['cleaned_label'].to_list()\n","\n","    text = sample_text.lower().split('.')\n","    #text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin search\n","    # matching\n","    \n","    for known_label in existing_labels:   # for each label in the known set\n","        # EXACT MATCH\n","        if known_label in sample_text.lower():   # find the EXACT label in text \n","            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n","            \n","    for sentence in text:\n","        doc = nlp(text_cleaning(sentence))\n","        #doc = nlp2(text_cleaning(sentence))\n","\n","        for entity in doc.ents:\n","            if entity.label_ == 'DATASET':\n","                cleaned_labels.append(clean_text(entity.text))   \n","            \n","        # CASE 1: FUZZY MATCH\n","        #value = fuzz.partial_ratio(sentence.lower(), known_label) # I moved .lower() here\n","        #if value > 85 and value < 100:\n","            # print('value: ', str(value), known_label) # Alex, you might wanna see what this prints\n","            # cleaned_labels.append(clean_text(known_label))\n","    \n","        # CASE 2: for unknown labels\n","        # sentence filtering (Longest Consecutive Capitalization)\n","        #print(sentence)\n","#             length, rate, filtered_sentence = LCC(sentence)\n","#             if rate <= 0 or length == 0 or (length == 1 and not sentence.isupper()): \n","#                 continue # no consecutive caps found\n","#             # <insert classifier here>\n","#             else:\n","#                 for keyword in [\"dataset\", \"data\", \"database\", \"survey\", \"study\", \"research\", \"statistics\"]:\n","#                     if keyword in filtered_sentence.lower():\n","#                         #pass\n","#                         cleaned_labels.append(clean_text(filtered_sentence)) # naive\n","        \n","    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n","    cleaned_labels = set(cleaned_labels)\n","    lables_list.append('|'.join(cleaned_labels))\n","    id_list.append(row_id)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.DataFrame()\n","submission['Id'] = id_list\n","submission['PredictionString'] = lables_list"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","submission.head()"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for item in submission[\"PredictionString\"]:\n","#    print(item)\n","#    print()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":null,"outputs":[]}]}