{"metadata":{"kernelspec":{"name":"python388jvsc74a57bd006f0b86e6bdba99e28d98577ff6ec36020717253ecd624b165753310a3730b6d","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import csv\n","import glob\n","import random\n","from collections import defaultdict\n","\n","from functools import partial\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","\n","import nltk\n","import string\n","\n","# spaCy \n","import spacy\n","from spacy.util import minibatch, compounding\n","#from spacy.training.example import Example # version 3 only\n","\n","KAGGLE = False"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":1,"outputs":[]},{"source":["## Preprocessing"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["if KAGGLE:\n","    from model.model import longest_consecutive_caps as LCC\n","    from model.model import KMP\n","\n","    train_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\n","    sample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n","    train_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\n","    test_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\"\n","    gvnt_dataset_path = \"/kaggle/input/bigger-govt-dataset-list/data_set_800.csv\"\n","\n","else:\n","    from model import longest_consecutive_caps as LCC\n","    from model import KMP\n","    \n","    train_df = pd.read_csv(\"dataset/train.csv\")\n","    sample_sub = pd.read_csv('dataset/sample_submission.csv')\n","    train_fp = \"dataset/train/\"\n","    test_fp = \"dataset/test/\"\n","    gvnt_dataset_path = \"dataset/gvnt_800.csv\""],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#!python -m spacy download en_core_web_trf"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def read_append_return(filename, train_files_path=train_fp, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \n","train_df['text'] = train_df['Id'].progress_apply(read_append_return)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 19661/19661 [00:18<00:00, 1056.00it/s]CPU times: user 7.37 s, sys: 1.97 s, total: 9.34 s\n","Wall time: 18.6 s\n","\n"]}]},{"cell_type":"code","source":["def text_cleaning(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    #text = re.sub(' +', ' ', str(text).lower()).strip()\n","    text = re.sub('[^A-Za-z0-9 ]+', '|', str(text).lower())\n","    #text = ''.join([k for k in text if k not in string.punctuation])\n","    #text = re.sub('[^A-Za-z0-9.]+', ' ', str(text).lower()).strip()\n","#     text = re.sub(\"/'+/g\", ' ', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","train_df['text'] = train_df['text'].progress_apply(text_cleaning)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 19661/19661 [00:35<00:00, 553.98it/s] CPU times: user 34.6 s, sys: 1.12 s, total: 35.7 s\n","Wall time: 35.5 s\n","\n"]}]},{"cell_type":"code","source":["a = train_df[\"Id\"].nunique()\n","b = train_df[\"Id\"].size\n","print(a, b)\n","print(a/b)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["14316 19661\n0.7281420070189716\n"]}]},{"cell_type":"code","source":["#train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":9,"outputs":[]},{"source":["## Training\n","### Choose spaCy model and prepare training data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# data configurations\n","\n","# select model backbone\n","model = None \n","#model = \"specified\" # specified for transformer + ner only\n","#model = \"generic\" # a model based on existing en_core_web_sm\n","\n","token_anno = \"entities\" # note this may require changes for v3.0+ transformer models\n","negative_sample = True # False: only prepares X_1; True: also prepares X_0\n","neg_sample_rate = 0.005"]},{"cell_type":"code","source":["TRAIN_DATA = []\n","x1, x2, x3, x4 = 0, 0, 0, 0\n","\n","print(\"Preparing training data...\")\n","\n","for index, row in tqdm(train_df.iterrows()):\n","    # get text of each sample test\n","    train_text = row['text']\n","    row_id = row['Id']\n","    label = row['cleaned_label']\n","    m = len(label)\n","    text = train_text.lower().split('|')\n","    #text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin matching\n","    for sentence in text:\n","        x1 += 1\n","        # clean text \n","        #sentence = clean_text(sentence)\n","        indexed = KMP(label, sentence)\n","        if indexed != []:\n","            n = len(sentence)\n","            elist = []\n","            for i in indexed:\n","                end = m + i\n","                a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n","                b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n","                if a and b:\n","                    entity = (i, end, \"DATASET\") \n","                    elist.append(entity)\n","                    x3 += 1\n","            if elist == []: break # TEST (REMOVE IF NEEDED)\n","            x = (sentence, {token_anno:elist})\n","            TRAIN_DATA.append(x)\n","        \n","        elif negative_sample: \n","            x2 += 1\n","            if random.random() < neg_sample_rate:\n","                x4 += 1\n","                TRAIN_DATA.append((sentence, {token_anno:[]})) # TEST"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["9it [00:00, 85.75it/s]Preparing training data...\n","19661it [06:42, 48.81it/s]\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]\nAmong 35961857 sentences,\n35908537 sentences have no title in them,\nmeaning 99.852% do not have title.\nSo we have 53320 sentences with titles.\nIn 53320 sentences, we obtained 53506 tokens (positive samples) that perfectly match titles.\nFor sentences without any title, we chose 179213 negative samples.\nThe ratio of positive vs negative samples is 29.86 : 100.\n"]}],"source":["print(\"[INFO]\")\n","print(\"Among %d sentences,\"%x1)\n","print(\"%d sentences have no title in them,\"%x2)\n","print(\"meaning %.3f%% do not have title.\"%(x2/x1*100))\n","print(\"So we have %d sentences with titles.\"%(x1-x2))\n","print(\"In %d sentences, we obtained %d tokens (positive samples) that perfectly match titles.\"%(x1-x2, x3))\n","print(\"For sentences without any title, we chose %d negative samples.\"%x4)\n","print(\"The ratio of positive vs negative samples is %.2f : 100.\"%(x3 / x4 * 100))\n"]},{"source":["### Configure training pipeline and train the spaCy model\n","\n","Download official model with `python3 -m spacy download en_core_web_sm`"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# training configurations\n","output_dir = \"output/\"\n","n_iter = 15 # number of training iteration"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU: True\n","Created blank 'en' model\n"]}],"source":["print(\"Using GPU:\", spacy.prefer_gpu())\n","\n","# load the model\n","if model == \"specified\":\n","    nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","elif model == \"generic\":\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    print(\"Loaded generic model en_core_web_sm\")\n","elif model is not None:\n","    nlp = spacy.load(model)  \n","    print(\"Loaded model '%s'\" % model)\n","else:\n","    nlp = spacy.blank('en')  \n","    print(\"Created blank 'en' model\")\n","\n","# add ner component to pipeline\n","if 'ner' not in nlp.pipe_names:\n","    ner = nlp.create_pipe('ner') # version < 3.0\n","    nlp.add_pipe(ner, last=True) # verions < 3.0\n","    #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n","    #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\n","else:\n","    ner = nlp.get_pipe('ner')\n","\n","# add all labels to the ner\n","#for _, annotations in TRAIN_DATA:\n","#    for ent in annotations.get('entities'):\n","#        ner.add_label(ent[2])\n","ner.add_label(\"DATASET\") # only one category so no for loop\n","\n","# configure optimizer (may not work with spaCy v3 transformers)\n","optimizer = nlp.begin_training()\n","\n","# configure pipeline components to disable\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"]},{"cell_type":"code","source":["# start training\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","    for itn in range(n_iter):\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001)) # HYPERPARAMETERS\n","        # version < 3.0 use this\n","        for batch in tqdm(batches):\n","            texts, annotations = zip(*batch)\n","            nlp.update(\n","                texts, # or [texts] if not using batch\n","                annotations, # or [annotations] if not using batch\n","                drop=0.5,  \n","                sgd=optimizer,\n","                losses=losses)\n","        \n","        # For spaCy v3.0+ use the similar for loops in archived.ipynb\n","            \n","        print(\"\\titeration %d, ner loss: %.2f\"%(itn+1, losses['ner']))"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["8482it [05:01, 28.12it/s]\n","4it [00:00, 38.36it/s]\titeration 1, ner loss: 15009.39\n","8482it [04:44, 29.86it/s]\n","4it [00:00, 33.96it/s]\titeration 2, ner loss: 10123.50\n","8482it [04:44, 29.81it/s]\n","4it [00:00, 37.12it/s]\titeration 3, ner loss: 9707.54\n","8482it [04:44, 29.83it/s]\n","5it [00:00, 40.05it/s]\titeration 4, ner loss: 9103.64\n","8482it [05:08, 27.48it/s]\n","4it [00:00, 35.63it/s]\titeration 5, ner loss: 8826.16\n","8482it [04:48, 29.36it/s]\n","4it [00:00, 33.44it/s]\titeration 6, ner loss: 8737.68\n","8482it [04:44, 29.78it/s]\n","4it [00:00, 37.66it/s]\titeration 7, ner loss: 8402.84\n","8482it [04:44, 29.78it/s]\n","4it [00:00, 38.06it/s]\titeration 8, ner loss: 8566.76\n","8482it [04:45, 29.75it/s]\n","4it [00:00, 37.30it/s]\titeration 9, ner loss: 8626.34\n","8482it [04:45, 29.75it/s]\n","4it [00:00, 38.13it/s]\titeration 10, ner loss: 8629.01\n","8482it [04:45, 29.74it/s]\n","4it [00:00, 39.04it/s]\titeration 11, ner loss: 8651.18\n","8482it [04:45, 29.74it/s]\n","0it [00:00, ?it/s]\titeration 12, ner loss: 8835.89\n","8482it [04:45, 29.69it/s]\n","4it [00:00, 38.34it/s]\titeration 13, ner loss: 8757.40\n","8482it [04:45, 29.76it/s]\n","4it [00:00, 37.76it/s]\titeration 14, ner loss: 8550.09\n","8482it [04:45, 29.75it/s]\titeration 15, ner loss: 8620.40\n","\n"]}]},{"source":["## Testing and Exporting Model"],"cell_type":"markdown","metadata":{}},{"source":["# sample test\n","#i = 0\n","#for text, _ in TRAIN_DATA:\n","#    doc = nlp(text)\n","#    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n","#    i += 1\n","#    if i > 50: break\n","\n","# save model\n","if output_dir is not None:\n","    output_dir = Path(output_dir)\n","    if not output_dir.exists():\n","        output_dir.mkdir()\n","    nlp.to_disk(output_dir)\n","    print(\"Saved model to\", output_dir)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities [('nces common core of data', 'DATASET')]\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities [('adni', 'DATASET')]\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities []\n","Entities [('ibtracs', 'DATASET')]\n","Entities []\n","Entities []\n","Entities []\n","Entities [('slosh model', 'DATASET')]\n","Entities []\n","Entities [('adni', 'DATASET')]\n","Entities [('agricultural resource management survey', 'DATASET')]\n","Entities []\n","Entities []\n","Entities [('adni', 'DATASET')]\n","Entities []\n","Entities []\n","Saved model to output\n"]}]},{"source":["## Inference with Trained Model\n","Generates the submission file"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def clean_text(txt):\n","    ''' DO NOT DELETE: Official function for submission text cleaning '''\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 4/4 [00:00<00:00, 1717.04it/s]CPU times: user 5.78 ms, sys: 9 µs, total: 5.78 ms\n","Wall time: 4.62 ms\n","\n"]}]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["def load_gvnt_dataset():\n","    with open(gvnt_dataset_path) as f:\n","        reader = csv.reader(f)\n","        my_list = list(reader)\n","    dataset = [row[0] for row in my_list][1:]\n","    return dataset"]},{"cell_type":"code","source":["# STRING MATCHING BLOCK\n","temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n","temp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\n","temp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n","gvnt_set = load_gvnt_dataset()\n","\n","existing_labels = set(temp_1 + temp_2 + temp_3 + gvnt_set)\n","id_list = []\n","lables_list = []\n","\n","# load model 'EN'\n","#nlp = spacy.load(output_dir) # loading an model can be slower?\n","#other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n","\n","for index, row in tqdm(sample_sub.iterrows()):\n","    # get text of each sample test\n","    sample_text = row['text']\n","    row_id = row['Id']\n","    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n","\n","    cleaned_labels = temp_df['cleaned_label'].to_list()\n","\n","    texts = sample_text.lower().split('.')\n","    #text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin search\n","    # matching\n","    \n","    for known_label in existing_labels:   # for each label in the known set\n","        # EXACT MATCH\n","        if known_label in sample_text.lower():   # find the EXACT label in text \n","            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n","    \n","    # THIS METHOD BELOW IS MUCH FASTER!!!!!\n","    # SOURCE: https://spacy.io/usage/processing-pipelines\n","    # SOURCE 2: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n","    # THANK YOU SOOO MUCH!!!\n","    # Disabling pipeline components also helps\n","    for doc in nlp.pipe(texts, disable=other_pipes):\n","        cleaned_labels.extend([clean_text(entity.text) for entity in doc.ents if entity.label_ == \"DATASET\"])\n","    \n","\n","    #for sentence in texts:   \n","        #doc = nlp(text_cleaning(sentence))\n","    #    doc = nlp2(text_cleaning(sentence))\n","\n","    #    for entity in doc.ents:\n","    #        if entity.label_ == 'DATASET':\n","    #            cleaned_labels.append(clean_text(entity.text))   \n","            \n","        # CASE 1: FUZZYMATCH (see archived)\n","\n","        # CASE 2: Consecutive Capitalizations (see archived)\n","        \n","        \n","    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n","    cleaned_labels = set(cleaned_labels)\n","    lables_list.append('|'.join(cleaned_labels))\n","    id_list.append(row_id)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["4it [00:04,  1.08s/it]\n"]}]},{"cell_type":"code","source":["submission = pd.DataFrame()\n","submission['Id'] = id_list\n","submission['PredictionString'] = lables_list"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","submission.head()"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     Id  \\\n","0  2100032a-7c33-4bff-97ef-690822c43466   \n","1  2f392438-e215-4169-bebf-21ac4ff253e1   \n","2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n","3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n","\n","                                    PredictionString  \n","0  adni|alzheimer s disease neuroimaging initiati...  \n","1  common core of data|nces common core of data|s...  \n","2  slosh model|slosh display program|slosh and|sl...  \n","3                        rural urban continuum codes  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>adni|alzheimer s disease neuroimaging initiati...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>common core of data|nces common core of data|s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>slosh model|slosh display program|slosh and|sl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>rural urban continuum codes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["for item in submission[\"PredictionString\"]:\n","    print(item)\n","    print()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["adni|alzheimer s disease neuroimaging initiative adni |alzheimer s disease neuroimaging initiative adni|pubmed\n\ncommon core of data|nces common core of data|schools and staffing survey|trends in international mathematics and science study|integrated postsecondary education data system|progress in international reading literacy study|ipeds|education 3 to|program focusing on research and taken\n\nslosh model|slosh display program|slosh and|slosh moms|8 00|coastal observation station|coastal change science along|noaa storm surge inundation|slosh point|sea lake and overland surges from hurricanes|slosh meows|slosh display\n\nrural urban continuum codes\n\n"]}]}]}