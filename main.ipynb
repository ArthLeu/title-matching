{"metadata":{"kernelspec":{"name":"python388jvsc74a57bd006f0b86e6bdba99e28d98577ff6ec36020717253ecd624b165753310a3730b6d","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import csv\n","import glob\n","import random\n","import time\n","from collections import defaultdict\n","\n","from functools import partial\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","\n","import nltk\n","import string\n","\n","# spaCy \n","import spacy\n","from spacy.util import minibatch, compounding\n","#from spacy.training.example import Example # version 3 only"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# choose how to run\n","KAGGLE = False\n","TRAINING = False"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["start_time = time.time()"]},{"source":["## Preprocessing"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["if KAGGLE:\n","    from model.model import longest_consecutive_caps as LCC\n","    from model.model import KMP\n","\n","    train_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\n","    sample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n","    train_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\n","    test_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\"\n","    gvnt_dataset_path = \"/kaggle/input/bigger-govt-dataset-list/data_set_800.csv\"\n","\n","    model_dir = \"/kaggle/input/pretrained-models/output/\" # where to store trained model or load pretrained model\n","\n","else:\n","    from model import longest_consecutive_caps as LCC\n","    from model import KMP\n","    \n","    train_df = pd.read_csv(\"dataset/train.csv\")\n","    sample_sub = pd.read_csv('dataset/sample_submission.csv')\n","    train_fp = \"dataset/train/\"\n","    test_fp = \"dataset/test/\"\n","    gvnt_dataset_path = \"dataset/gvnt_800.csv\"\n","\n","    model_dir = \"output/\""],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#!python -m spacy download en_core_web_trf"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def read_append_return(filename, train_files_path=train_fp, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \n","train_df['text'] = train_df['Id'].progress_apply(read_append_return)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 19661/19661 [00:05<00:00, 3582.95it/s]CPU times: user 4.57 s, sys: 940 ms, total: 5.51 s\n","Wall time: 5.49 s\n","\n"]}]},{"cell_type":"code","source":["def text_cleaning(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    #text = re.sub(' +', ' ', str(text).lower()).strip()\n","    text = re.sub('[^A-Za-z0-9 ]+', '|', str(text).lower())\n","    #text = ''.join([k for k in text if k not in string.punctuation])\n","    #text = re.sub('[^A-Za-z0-9.]+', ' ', str(text).lower()).strip()\n","#     text = re.sub(\"/'+/g\", ' ', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","train_df['text'] = train_df['text'].progress_apply(text_cleaning)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 19661/19661 [00:36<00:00, 532.04it/s] CPU times: user 35.7 s, sys: 1.44 s, total: 37.1 s\n","Wall time: 37 s\n","\n"]}]},{"cell_type":"code","source":["a = train_df[\"Id\"].nunique()\n","b = train_df[\"Id\"].size\n","print(a, b)\n","print(a/b)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["14316 19661\n0.7281420070189716\n"]}]},{"cell_type":"code","source":["#train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"source":["## Training\n","If `TRAINING` is enabled, choose spaCy model and prepare training data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# TRAINING CONFIGURATIONS\n","\n","# select model backbone\n","model = None \n","#model = \"specified\"            # specified for transformer + ner only\n","#model = \"generic\"              # a model based on existing en_core_web_sm\n","\n","# data preparation configurations\n","token_anno = \"entities\"         # note this may require changes for v3.0+ transformer models\n","negative_sample = True          # False: only prepares X_1; True: also prepares X_0\n","neg_sample_rate = 0.005         # only this portion of negative samples will be added to training set\n","\n","# training configurations\n","n_iter = 15 # number of training iteration\n","minibatch_min = 4.0\n","minibatch_max = 32.0\n","minibatch_int = 1.001\n","droprate = 0.5"]},{"cell_type":"code","source":["if TRAINING:\n","    TRAIN_DATA = []\n","    x1, x2, x3, x4 = 0, 0, 0, 0\n","\n","    print(\"Preparing training data...\")\n","\n","    for index, row in tqdm(train_df.iterrows()):\n","        # get text of each sample test\n","        train_text = row['text']\n","        row_id = row['Id']\n","        label = row['cleaned_label']\n","        m = len(label)\n","        text = train_text.lower().split('|')\n","        #text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","        # begin matching\n","        for sentence in text:\n","            x1 += 1\n","            # clean text \n","            #sentence = clean_text(sentence)\n","            indexed = KMP(label, sentence)\n","            if indexed != []:\n","                n = len(sentence)\n","                elist = []\n","                for i in indexed:\n","                    end = m + i\n","                    a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n","                    b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n","                    if a and b:\n","                        entity = (i, end, \"DATASET\") \n","                        elist.append(entity)\n","                        x3 += 1\n","                if elist == []: break # TEST (REMOVE IF NEEDED)\n","                x = (sentence, {token_anno:elist})\n","                TRAIN_DATA.append(x)\n","            \n","            elif negative_sample: \n","                x2 += 1\n","                if random.random() < neg_sample_rate:\n","                    x4 += 1\n","                    TRAIN_DATA.append((sentence, {token_anno:[]})) # TEST"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["if TRAINING:\n","    print(\"[INFO]\")\n","    print(\"Among %d sentences,\"%x1)\n","    print(\"%d sentences have no title in them,\"%x2)\n","    print(\"meaning %.3f%% do not have title.\"%(x2/x1*100))\n","    print(\"So we have %d sentences with titles.\"%(x1-x2))\n","    print(\"In %d sentences, we obtained %d tokens (positive samples) that perfectly match titles.\"%(x1-x2, x3))\n","    print(\"For sentences without any title, we chose %d negative samples.\"%x4)\n","    print(\"The ratio of positive vs negative samples is %.2f : 100.\"%(x3 / x4 * 100))"]},{"source":["### Configure training pipeline and train the spaCy model\n","\n","Download official model with `python3 -m spacy download en_core_web_sm`"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["if TRAINING:\n","    print(\"Using GPU:\", spacy.prefer_gpu())\n","\n","    # load the model\n","    if model == \"specified\":\n","        nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","    elif model == \"generic\":\n","        nlp = spacy.load(\"en_core_web_sm\")\n","        print(\"Loaded generic model en_core_web_sm\")\n","    elif model is not None:\n","        nlp = spacy.load(model)  \n","        print(\"Loaded model '%s'\" % model)\n","    else:\n","        nlp = spacy.blank('en')  \n","        print(\"Created blank 'en' model\")\n","\n","    # add ner component to pipeline\n","    if 'ner' not in nlp.pipe_names:\n","        ner = nlp.create_pipe('ner') # version < 3.0\n","        nlp.add_pipe(ner, last=True) # verions < 3.0\n","        #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n","        #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\n","    else:\n","        ner = nlp.get_pipe('ner')\n","\n","    # add all labels to the ner\n","    #for _, annotations in TRAIN_DATA:\n","    #    for ent in annotations.get('entities'):\n","    #        ner.add_label(ent[2])\n","    ner.add_label(\"DATASET\") # only one category so no for loop\n","\n","    # configure optimizer (may not work with spaCy v3 transformers)\n","    optimizer = nlp.begin_training()\n","\n","    # configure pipeline components to disable\n","    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"]},{"cell_type":"code","source":["if TRAINING:\n","    # start training\n","    with nlp.disable_pipes(*other_pipes):  # only train NER\n","        for itn in range(n_iter):\n","            random.shuffle(TRAIN_DATA)\n","            losses = {}\n","            batches = minibatch(TRAIN_DATA, size=compounding(minibatch_min, minibatch_max, minibatch_int)) # HYPERPARAMETERS\n","            # version < 3.0 use this\n","            for batch in tqdm(batches):\n","                texts, annotations = zip(*batch)\n","                nlp.update(\n","                    texts, # or [texts] if not using batch\n","                    annotations, # or [annotations] if not using batch\n","                    drop=droprate,  \n","                    sgd=optimizer,\n","                    losses=losses)\n","            \n","            # For spaCy v3.0+ use the similar for loops in archived.ipynb\n","                \n","            print(\"\\titeration %d, ner loss: %.2f\"%(itn+1, losses['ner']))"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":16,"outputs":[]},{"source":["## Testing and Exporting Model"],"cell_type":"markdown","metadata":{}},{"source":["if TRAINING:\n","    # sample test\n","    #i = 0\n","    #for text, _ in TRAIN_DATA:\n","    #    doc = nlp(text)\n","    #    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n","    #    i += 1\n","    #    if i > 50: break\n","\n","    # save model\n","    if model_dir is not None:\n","        model_dir = Path(model_dir)\n","        if not model_dir.exists():\n","            model_dir.mkdir()\n","        nlp.to_disk(model_dir)\n","        print(\"Saved model to\", model_dir)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":17,"outputs":[]},{"source":["## Inference with Trained Model\n","Generates the submission file"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def clean_text(txt):\n","    ''' DO NOT DELETE: Official function for submission text cleaning '''\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 1115.65it/s]CPU times: user 7.79 ms, sys: 325 µs, total: 8.12 ms\n","Wall time: 6.98 ms\n","\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def load_gvnt_dataset():\n","    with open(gvnt_dataset_path) as f:\n","        reader = csv.reader(f)\n","        my_list = list(reader)\n","    dataset = [row[0] for row in my_list][1:]\n","    return dataset"]},{"cell_type":"code","source":["# STRING MATCHING BLOCK\n","temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n","temp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\n","temp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n","gvnt_set = load_gvnt_dataset()\n","\n","existing_labels = set(temp_1 + temp_2 + temp_3 + gvnt_set)\n","id_list = []\n","lables_list = []\n","\n","# load model 'EN'\n","if not TRAINING:\n","    print(\"Using GPU:\", spacy.prefer_gpu())\n","    nlp = spacy.load(model_dir) # loading an model can be slower?\n","    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n","\n","\n","for index, row in tqdm(sample_sub.iterrows()):\n","    # get text of each sample test\n","    sample_text = row['text']\n","    row_id = row['Id']\n","    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n","\n","    cleaned_labels = temp_df['cleaned_label'].to_list()\n","\n","    texts = sample_text.lower().split('.')\n","    #text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin search\n","    # matching\n","    \n","    for known_label in existing_labels:   # for each label in the known set\n","        # EXACT MATCH\n","        if known_label in sample_text.lower():   # find the EXACT label in text \n","            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n","    \n","    # THIS METHOD BELOW IS MUCH FASTER!!!!!\n","    # SOURCE: https://spacy.io/usage/processing-pipelines\n","    # SOURCE 2: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n","    # THANK YOU SOOO MUCH!!!\n","    # Disabling pipeline components also helps\n","    for doc in nlp.pipe(texts, disable=other_pipes):\n","        cleaned_labels.extend([clean_text(entity.text) for entity in doc.ents if entity.label_ == \"DATASET\"])\n","    \n","\n","    #for sentence in texts:   \n","        #doc = nlp(text_cleaning(sentence))\n","    #    doc = nlp2(text_cleaning(sentence))\n","\n","    #    for entity in doc.ents:\n","    #        if entity.label_ == 'DATASET':\n","    #            cleaned_labels.append(clean_text(entity.text))   \n","            \n","        # CASE 1: FUZZYMATCH (see archived)\n","\n","        # CASE 2: Consecutive Capitalizations (see archived)\n","        \n","        \n","    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n","    cleaned_labels = set(cleaned_labels)\n","    lables_list.append('|'.join(cleaned_labels))\n","    id_list.append(row_id)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU: True\n","4it [00:05,  1.33s/it]\n"]}]},{"cell_type":"code","source":["submission = pd.DataFrame()\n","submission['Id'] = id_list\n","submission['PredictionString'] = lables_list"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","submission.head()"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     Id  \\\n","0  2100032a-7c33-4bff-97ef-690822c43466   \n","1  2f392438-e215-4169-bebf-21ac4ff253e1   \n","2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n","3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n","\n","                                    PredictionString  \n","0  alzheimer s disease neuroimaging initiative ad...  \n","1  education 3 to|nces common core of data|trends...  \n","2  slosh display program|8 00|slosh and|slosh poi...  \n","3                        rural urban continuum codes  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>alzheimer s disease neuroimaging initiative ad...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>education 3 to|nces common core of data|trends...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>slosh display program|8 00|slosh and|slosh poi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>rural urban continuum codes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#for item in submission[\"PredictionString\"]:\n","#    print(item)\n","#    print()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Total time spent: 61.73 seconds\n"]}],"source":["end_time = time.time()\n","print(\"Total time spent: %.2f seconds\"%(end_time-start_time))"]}]}