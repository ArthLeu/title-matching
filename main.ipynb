{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import glob\n","from collections import defaultdict\n","from functools import partial\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","#from tqdm.autonotebook import tqdm\n","\n","import nltk\n","import string\n","from fuzzywuzzy import fuzz\n","\n","from model import longest_consecutive_caps as LCC\n","from model import KMP\n","#from model.model import longest_consecutive_caps as LCC\n","#from model.model import KMP\n","\n","\n","# Spacy model\n","import spacy\n","#from __future__ import unicode_literals, print_function\n","#import plac\n","import random\n","from pathlib import Path\n","from tqdm import tqdm\n","#from spacy.training.example import Example # version 3 only\n","\n","\n","output_dir=\"output/\"\n","model = None \n","#model = \"specified\" # specified for transformer + ner only\n","n_iter = 10 # number of training iteration"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/anaconda3/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]}]},{"cell_type":"code","source":["#!python -m spacy download en_core_web_trf"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv(\"dataset/train.csv\")\n","sample_sub = pd.read_csv('dataset/sample_submission.csv')\n","train_fp = \"dataset/train/\"\n","test_fp = \"dataset/test/\"\n","\n","#train_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\n","#sample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n","#train_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\n","#test_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\""],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#train_df.head(5)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.030078Z","iopub.execute_input":"2021-06-02T08:59:11.030546Z","iopub.status.idle":"2021-06-02T08:59:11.034559Z","shell.execute_reply.started":"2021-06-02T08:59:11.030508Z","shell.execute_reply":"2021-06-02T08:59:11.033393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#train_df.info()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.038313Z","iopub.execute_input":"2021-06-02T08:59:11.038765Z","iopub.status.idle":"2021-06-02T08:59:11.045212Z","shell.execute_reply.started":"2021-06-02T08:59:11.038727Z","shell.execute_reply":"2021-06-02T08:59:11.044344Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def read_append_return(filename, train_files_path=train_fp, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \n","train_df['text'] = train_df['Id'].progress_apply(read_append_return)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 19661/19661 [00:06<00:00, 3200.50it/s]CPU times: user 5.01 s, sys: 1.13 s, total: 6.14 s\n","Wall time: 6.15 s\n","\n"]}]},{"cell_type":"code","source":["def text_cleaning(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    #text = re.sub(' +', ' ', str(text).lower()).strip()\n","    text = re.sub('[^A-Za-z0-9 ]+', '|', str(text).lower())\n","    #text = ''.join([k for k in text if k not in string.punctuation])\n","    #text = re.sub('[^A-Za-z0-9.]+', ' ', str(text).lower()).strip()\n","#     text = re.sub(\"/'+/g\", ' ', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","train_df['text'] = train_df['text'].progress_apply(text_cleaning)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 19661/19661 [00:43<00:00, 451.58it/s]CPU times: user 42.4 s, sys: 1.24 s, total: 43.7 s\n","Wall time: 43.6 s\n","\n"]}]},{"cell_type":"code","source":["a = train_df[\"Id\"].nunique()\n","b = train_df[\"Id\"].size\n","print(a, b)\n","print(a/b)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["14316 19661\n0.7281420070189716\n"]}]},{"cell_type":"code","source":["#train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"source":["### Prepare training data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["TRAIN_DATA = []\n","if model == \"specified\":\n","    token_anno = \"entities\"\n","else:\n","    token_anno = \"entities\"\n","\n","\n","for index, row in tqdm(train_df.iterrows()):\n","    # get text of each sample test\n","    train_text = row['text']\n","    row_id = row['Id']\n","    label = row['cleaned_label']\n","    m = len(label)\n","    text = train_text.lower().split('|')\n","    #text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin matching\n","    for sentence in text:\n","        # clean text \n","        #sentence = clean_text(sentence)\n","        indexed = KMP(label, sentence)\n","        if indexed != []:\n","            n = len(sentence)\n","            elist = []\n","            for i in indexed:\n","                end = m+i\n","                a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n","                b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n","                if a and b:\n","                    entity = (i, end, \"DATASET\") \n","                    elist.append(entity)\n","            if elist == []: break # TEST (REMOVE IF NEEDED)\n","            x = (sentence, {token_anno:elist})\n","            TRAIN_DATA.append(x)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["19661it [08:02, 40.76it/s]\n"]}]},{"source":["### Train the spaCy model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["from spacy.util import minibatch, compounding\n","#load the model\n","n_iter = 25 # number of training iteration (overriding n_iter declared in cell 1)\n","print(spacy.prefer_gpu())\n","\n","if model == \"specified\":\n","    nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","elif model == \"generic\":\n","    nlp = spacy.load(\"en_core_web_sm\")\n","elif model is not None:\n","    nlp = spacy.load(model)  \n","    print(\"Loaded model '%s'\" % model)\n","else:\n","    nlp = spacy.blank('en')  \n","    print(\"Created blank 'en' model\")\n","\n","\n","#set up the pipeline\n","if 'ner' not in nlp.pipe_names:\n","    ner = nlp.create_pipe('ner') # version < 3.0\n","    nlp.add_pipe(ner, last=True) # verions < 3.0\n","    #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n","    #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\n","else:\n","    ner = nlp.get_pipe('ner')\n","\n","#for _, annotations in TRAIN_DATA:\n","#    for ent in annotations.get('entities'):\n","#        ner.add_label(ent[2])\n","ner.add_label(\"DATASET\") # only one category so no for loop\n","\n","\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","    if model == None: \n","        optimizer = nlp.begin_training()\n","    for itn in range(n_iter):\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001)) # HYPERPARAMETERS\n","        for batch in tqdm(batches):\n","            texts, annotations = zip(*batch)\n","            nlp.update(\n","                texts, # or [texts] if not using batch\n","                annotations, # or [annotations] if not using batch\n","                drop=0.5,  \n","                sgd=optimizer,\n","                losses=losses)\n","        #for batch in tqdm(batches):\n","        #for texts, annotations in tqdm(TRAIN_DATA):\n","        #    for texts, annotations in batch:\n","        #        doc = nlp.make_doc(texts) # version >= 3\n","        #        example = Example.from_dict(doc, annotations) # version >= 3\n","        #        nlp.update(\n","        #            [example],\n","        #            drop=0.5, \n","        #            sgd=optimizer,\n","        #            losses=losses) # version >= 3\n","            \n","        print(\"\\niteration\", itn+1,\"ner loss\", round(losses['ner'], 3))"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","Created blank 'en' model\n","/home/ry/anaconda3/lib/python3.8/site-packages/spacy/language.py:635: UserWarning: [W033] Training a new parser or NER using a model with an empty lexeme normalization table. This may degrade the performance to some degree. If this is intentional or this language doesn't have a normalization table, please ignore this warning.\n","  proc.begin_training(\n","/home/ry/anaconda3/lib/python3.8/site-packages/spacy/language.py:635: UserWarning: [W034] Please install the package spacy-lookups-data in order to include the default lexeme normalization table for the language 'en'.\n","  proc.begin_training(\n","2882it [01:52, 25.66it/s]\n","3it [00:00, 26.44it/s]\n","iteration 1 ner loss 13404.515\n","2882it [01:51, 25.94it/s]\n","3it [00:00, 29.84it/s]\n","iteration 2 ner loss 8051.641\n","2882it [01:51, 25.94it/s]\n","3it [00:00, 25.31it/s]\n","iteration 3 ner loss 7739.962\n","2882it [01:50, 25.97it/s]\n","3it [00:00, 28.08it/s]\n","iteration 4 ner loss 7509.122\n","2882it [01:47, 26.73it/s]\n","4it [00:00, 31.12it/s]\n","iteration 5 ner loss 7297.483\n","2882it [01:48, 26.59it/s]\n","3it [00:00, 27.98it/s]\n","iteration 6 ner loss 7195.442\n","2882it [01:42, 28.06it/s]\n","3it [00:00, 29.20it/s]\n","iteration 7 ner loss 7043.471\n","2882it [01:42, 28.02it/s]\n","4it [00:00, 33.02it/s]\n","iteration 8 ner loss 6935.929\n","2882it [01:39, 28.93it/s]\n","4it [00:00, 34.58it/s]\n","iteration 9 ner loss 6658.808\n","2882it [01:35, 30.06it/s]\n","4it [00:00, 32.36it/s]\n","iteration 10 ner loss 6725.302\n","2882it [01:36, 30.00it/s]\n","4it [00:00, 34.12it/s]\n","iteration 11 ner loss 6642.548\n","2882it [01:36, 29.97it/s]\n","4it [00:00, 35.54it/s]\n","iteration 12 ner loss 6624.994\n","2882it [01:35, 30.09it/s]\n","3it [00:00, 29.46it/s]\n","iteration 13 ner loss 6583.864\n","2882it [01:36, 29.96it/s]\n","4it [00:00, 34.92it/s]\n","iteration 14 ner loss 6370.941\n","2882it [01:49, 26.37it/s]\n","3it [00:00, 27.86it/s]\n","iteration 15 ner loss 6381.967\n","2882it [01:51, 25.74it/s]\n","3it [00:00, 28.16it/s]\n","iteration 16 ner loss 6240.846\n","2882it [01:49, 26.26it/s]\n","3it [00:00, 29.30it/s]\n","iteration 17 ner loss 6321.208\n","2882it [01:39, 28.89it/s]\n","4it [00:00, 32.25it/s]\n","iteration 18 ner loss 6229.896\n","2882it [01:37, 29.43it/s]\n","3it [00:00, 29.96it/s]\n","iteration 19 ner loss 6250.261\n","2882it [01:36, 29.95it/s]\n","4it [00:00, 37.44it/s]\n","iteration 20 ner loss 6331.669\n","2882it [01:35, 30.12it/s]\n","3it [00:00, 29.12it/s]\n","iteration 21 ner loss 6266.653\n","2882it [01:35, 30.21it/s]\n","4it [00:00, 33.73it/s]\n","iteration 22 ner loss 6271.193\n","2882it [01:35, 30.16it/s]\n","4it [00:00, 36.52it/s]\n","iteration 23 ner loss 6199.108\n","2882it [01:35, 30.20it/s]\n","4it [00:00, 35.98it/s]\n","iteration 24 ner loss 6185.325\n","2882it [01:35, 30.20it/s]\n","4it [00:00, 32.08it/s]\n","iteration 25 ner loss 6144.956\n","2882it [01:48, 26.63it/s]\n","3it [00:00, 29.05it/s]\n","iteration 26 ner loss 6239.246\n","2882it [01:45, 27.26it/s]\n","4it [00:00, 32.37it/s]\n","iteration 27 ner loss 6199.192\n","2882it [01:41, 28.46it/s]\n","4it [00:00, 34.14it/s]\n","iteration 28 ner loss 6217.881\n","2882it [01:42, 28.23it/s]\n","4it [00:00, 33.38it/s]\n","iteration 29 ner loss 6163.454\n","2882it [01:41, 28.28it/s]\n","4it [00:00, 34.39it/s]\n","iteration 30 ner loss 6311.186\n","2882it [01:41, 28.51it/s]\n","4it [00:00, 33.37it/s]\n","iteration 31 ner loss 6210.465\n","2882it [01:34, 30.37it/s]\n","4it [00:00, 35.96it/s]\n","iteration 32 ner loss 6092.232\n","2882it [01:37, 29.56it/s]\n","4it [00:00, 33.31it/s]\n","iteration 33 ner loss 6007.004\n","2882it [01:47, 26.69it/s]\n","3it [00:00, 29.29it/s]\n","iteration 34 ner loss 6062.515\n","2882it [01:43, 27.78it/s]\n","4it [00:00, 32.15it/s]\n","iteration 35 ner loss 6190.665\n","2882it [01:42, 28.11it/s]\n","4it [00:00, 34.68it/s]\n","iteration 36 ner loss 6182.284\n","2882it [01:43, 27.86it/s]\n","3it [00:00, 29.62it/s]\n","iteration 37 ner loss 6174.699\n","2882it [01:42, 28.15it/s]\n","4it [00:00, 31.05it/s]\n","iteration 38 ner loss 6095.871\n","2882it [01:42, 28.23it/s]\n","4it [00:00, 32.56it/s]\n","iteration 39 ner loss 6131.899\n","2882it [01:41, 28.50it/s]\n","4it [00:00, 33.32it/s]\n","iteration 40 ner loss 6121.554\n","2882it [01:35, 30.16it/s]\n","4it [00:00, 34.56it/s]\n","iteration 41 ner loss 6140.819\n","2882it [01:34, 30.36it/s]\n","4it [00:00, 35.96it/s]\n","iteration 42 ner loss 6139.523\n","2882it [01:35, 30.32it/s]\n","4it [00:00, 32.99it/s]\n","iteration 43 ner loss 6074.997\n","2882it [01:35, 30.30it/s]\n","4it [00:00, 34.10it/s]\n","iteration 44 ner loss 6029.676\n","2882it [01:35, 30.24it/s]\n","3it [00:00, 28.42it/s]\n","iteration 45 ner loss 6096.226\n","2882it [01:43, 27.75it/s]\n","3it [00:00, 28.76it/s]\n","iteration 46 ner loss 6153.874\n","2882it [01:46, 27.08it/s]\n","3it [00:00, 29.43it/s]\n","iteration 47 ner loss 6103.923\n","2882it [01:47, 26.84it/s]\n","3it [00:00, 28.26it/s]\n","iteration 48 ner loss 6102.756\n","2882it [01:48, 26.60it/s]\n","3it [00:00, 28.56it/s]\n","iteration 49 ner loss 6020.649\n","2882it [01:48, 26.65it/s]\n","iteration 50 ner loss 6073.588\n","\n"]}]},{"source":["# sample test\n","#for text, _ in TRAIN_DATA:\n","#    doc = nlp(text)\n","#    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n","#    break\n","\n","# save model\n","if output_dir is not None:\n","    output_dir = Path(output_dir)\n","    if not output_dir.exists():\n","        output_dir.mkdir()\n","    nlp.to_disk(output_dir)\n","    print(\"Saved model to\", output_dir)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved model to output\n"]}]},{"source":["### Test the model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def clean_text(txt):\n","    ''' DO NOT DELETE: Official function for submission text cleaning '''\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n","    #return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 4/4 [00:00<00:00, 1064.41it/s]CPU times: user 8.74 ms, sys: 305 µs, total: 9.05 ms\n","Wall time: 7.55 ms\n","\n"]}]},{"cell_type":"code","source":["# STRING MATCHING BLOCK\n","temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n","temp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\n","temp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n","\n","existing_labels = set(temp_1 + temp_2 + temp_3)\n","id_list = []\n","lables_list = []\n","# load model 'EN'\n","nlp2 = spacy.load(output_dir)\n","for index, row in tqdm(sample_sub.iterrows()):\n","    # get text of each sample test\n","    sample_text = row['text']\n","    row_id = row['Id']\n","    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n","\n","    cleaned_labels = temp_df['cleaned_label'].to_list()\n","\n","    text = sample_text.lower().split('.')\n","    #text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin search\n","    # matching\n","    \n","    for known_label in existing_labels:   # for each label in the known set\n","        # EXACT MATCH\n","        if known_label in sample_text.lower():   # find the EXACT label in text \n","            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n","            \n","    for sentence in text:\n","        doc = nlp2(text_cleaning(sentence))\n","\n","        for entity in doc.ents:\n","            if entity.label_ == 'DATASET':\n","                cleaned_labels.append(clean_text(entity.text))   \n","            \n","        # CASE 1: FUZZY MATCH\n","        #value = fuzz.partial_ratio(sentence.lower(), known_label) # I moved .lower() here\n","        #if value > 85 and value < 100:\n","            # print('value: ', str(value), known_label) # Alex, you might wanna see what this prints\n","            # cleaned_labels.append(clean_text(known_label))\n","    \n","        # CASE 2: for unknown labels\n","        # sentence filtering (Longest Consecutive Capitalization)\n","        #print(sentence)\n","#             length, rate, filtered_sentence = LCC(sentence)\n","#             if rate <= 0 or length == 0 or (length == 1 and not sentence.isupper()): \n","#                 continue # no consecutive caps found\n","#             # <insert classifier here>\n","#             else:\n","#                 for keyword in [\"dataset\", \"data\", \"database\", \"survey\", \"study\", \"research\", \"statistics\"]:\n","#                     if keyword in filtered_sentence.lower():\n","#                         #pass\n","#                         cleaned_labels.append(clean_text(filtered_sentence)) # naive\n","        \n","    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n","    cleaned_labels = set(cleaned_labels)\n","    lables_list.append('|'.join(cleaned_labels))\n","    id_list.append(row_id)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["4it [00:14,  3.74s/it]\n"]}]},{"cell_type":"code","source":["submission = pd.DataFrame()\n","submission['Id'] = id_list\n","submission['PredictionString'] = lables_list"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","submission.head()"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     Id  \\\n","0  2100032a-7c33-4bff-97ef-690822c43466   \n","1  2f392438-e215-4169-bebf-21ac4ff253e1   \n","2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n","3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n","\n","                                    PredictionString  \n","0  alzheimer s disease neuroimaging initiative ad...  \n","1  students in science literacy|common core of da...  \n","2  sea lake and overland surges from hurricanes|n...  \n","3                        rural urban continuum codes  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>alzheimer s disease neuroimaging initiative ad...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>students in science literacy|common core of da...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>sea lake and overland surges from hurricanes|n...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>rural urban continuum codes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#for item in submission[\"PredictionString\"]:\n","#    print(item)\n","#    print()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":22,"outputs":[]}]}