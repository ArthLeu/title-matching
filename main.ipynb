{"metadata":{"kernelspec":{"name":"python388jvsc74a57bd006f0b86e6bdba99e28d98577ff6ec36020717253ecd624b165753310a3730b6d","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import json\n","import glob\n","import random\n","from collections import defaultdict\n","\n","from functools import partial\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","\n","import nltk\n","import string\n","\n","# spaCy \n","import spacy\n","from spacy.util import minibatch, compounding\n","#from spacy.training.example import Example # version 3 only\n","\n","KAGGLE = False"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:07.451605Z","iopub.execute_input":"2021-06-02T08:59:07.452078Z","iopub.status.idle":"2021-06-02T08:59:10.914594Z","shell.execute_reply.started":"2021-06-02T08:59:07.451968Z","shell.execute_reply":"2021-06-02T08:59:10.913845Z"},"trusted":true},"execution_count":1,"outputs":[]},{"source":["## Preprocessing"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["if KAGGLE:\n","    from model.model import longest_consecutive_caps as LCC\n","    from model.model import KMP\n","\n","    train_df = pd.read_csv(\"/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv\")\n","    sample_sub = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n","    train_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/train/\"\n","    test_fp = \"/kaggle/input/coleridgeinitiative-show-us-the-data/test/\"\n","\n","else:\n","    from model import longest_consecutive_caps as LCC\n","    from model import KMP\n","    \n","    train_df = pd.read_csv(\"dataset/train.csv\")\n","    sample_sub = pd.read_csv('dataset/sample_submission.csv')\n","    train_fp = \"dataset/train/\"\n","    test_fp = \"dataset/test/\""],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.921174Z","iopub.execute_input":"2021-06-02T08:59:10.921681Z","iopub.status.idle":"2021-06-02T08:59:11.028689Z","shell.execute_reply.started":"2021-06-02T08:59:10.921644Z","shell.execute_reply":"2021-06-02T08:59:11.027932Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#!python -m spacy download en_core_web_trf"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:10.916147Z","iopub.execute_input":"2021-06-02T08:59:10.916457Z","iopub.status.idle":"2021-06-02T08:59:10.919912Z","shell.execute_reply.started":"2021-06-02T08:59:10.916423Z","shell.execute_reply":"2021-06-02T08:59:10.919146Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#train_df.head(5)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.030078Z","iopub.execute_input":"2021-06-02T08:59:11.030546Z","iopub.status.idle":"2021-06-02T08:59:11.034559Z","shell.execute_reply.started":"2021-06-02T08:59:11.030508Z","shell.execute_reply":"2021-06-02T08:59:11.033393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#train_df.info()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.038313Z","iopub.execute_input":"2021-06-02T08:59:11.038765Z","iopub.status.idle":"2021-06-02T08:59:11.045212Z","shell.execute_reply.started":"2021-06-02T08:59:11.038727Z","shell.execute_reply":"2021-06-02T08:59:11.044344Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def read_append_return(filename, train_files_path=train_fp, output='text'):\n","    \"\"\"\n","    Function to read json file and then return the text data from them and append to the dataframe\n","    \"\"\"\n","    json_path = os.path.join(train_files_path, (filename+'.json'))\n","    headings = []\n","    contents = []\n","    combined = []\n","    with open(json_path, 'r') as f:\n","        json_decode = json.load(f)\n","        for data in json_decode:\n","            headings.append(data.get('section_title'))\n","            contents.append(data.get('text'))\n","            combined.append(data.get('section_title'))\n","            combined.append(data.get('text'))\n","    \n","    all_headings = ' '.join(headings)\n","    all_contents = ' '.join(contents)\n","    all_data = '. '.join(combined)\n","    \n","    if output == 'text':\n","        return all_contents\n","    elif output == 'head':\n","        return all_headings\n","    else:\n","        return all_data"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.047477Z","iopub.execute_input":"2021-06-02T08:59:11.047942Z","iopub.status.idle":"2021-06-02T08:59:11.057014Z","shell.execute_reply.started":"2021-06-02T08:59:11.047908Z","shell.execute_reply":"2021-06-02T08:59:11.056169Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()   #tqdm is used to show any code running with a progress bar. \n","train_df['text'] = train_df['Id'].progress_apply(read_append_return)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:59:11.058262Z","iopub.execute_input":"2021-06-02T08:59:11.058651Z","iopub.status.idle":"2021-06-02T09:00:09.70764Z","shell.execute_reply.started":"2021-06-02T08:59:11.05861Z","shell.execute_reply":"2021-06-02T09:00:09.706978Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 19661/19661 [00:05<00:00, 3778.86it/s]CPU times: user 4.4 s, sys: 807 ms, total: 5.21 s\n","Wall time: 5.21 s\n","\n"]}]},{"cell_type":"code","source":["def text_cleaning(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    #text = re.sub(' +', ' ', str(text).lower()).strip()\n","    text = re.sub('[^A-Za-z0-9 ]+', '|', str(text).lower())\n","    #text = ''.join([k for k in text if k not in string.punctuation])\n","    #text = re.sub('[^A-Za-z0-9.]+', ' ', str(text).lower()).strip()\n","#     text = re.sub(\"/'+/g\", ' ', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.710447Z","iopub.execute_input":"2021-06-02T09:00:09.710693Z","iopub.status.idle":"2021-06-02T09:00:09.71794Z","shell.execute_reply.started":"2021-06-02T09:00:09.710667Z","shell.execute_reply":"2021-06-02T09:00:09.717206Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","train_df['text'] = train_df['text'].progress_apply(text_cleaning)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:00:09.721267Z","iopub.execute_input":"2021-06-02T09:00:09.721515Z","iopub.status.idle":"2021-06-02T09:01:10.907616Z","shell.execute_reply.started":"2021-06-02T09:00:09.721491Z","shell.execute_reply":"2021-06-02T09:01:10.906734Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 19661/19661 [00:34<00:00, 574.80it/s] CPU times: user 33.6 s, sys: 824 ms, total: 34.4 s\n","Wall time: 34.2 s\n","\n"]}]},{"cell_type":"code","source":["a = train_df[\"Id\"].nunique()\n","b = train_df[\"Id\"].size\n","print(a, b)\n","print(a/b)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.908933Z","iopub.execute_input":"2021-06-02T09:01:10.909299Z","iopub.status.idle":"2021-06-02T09:01:10.920551Z","shell.execute_reply.started":"2021-06-02T09:01:10.909261Z","shell.execute_reply":"2021-06-02T09:01:10.918982Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["14316 19661\n0.7281420070189716\n"]}]},{"cell_type":"code","source":["#train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.922451Z","iopub.execute_input":"2021-06-02T09:01:10.922824Z","iopub.status.idle":"2021-06-02T09:01:10.926895Z","shell.execute_reply.started":"2021-06-02T09:01:10.922786Z","shell.execute_reply":"2021-06-02T09:01:10.925895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"source":["## Training\n","### Choose spaCy model and prepare training data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# data configurations\n","\n","# select model backbone\n","model = None \n","#model = \"specified\" # specified for transformer + ner only\n","#model = \"generic\" # a model based on existing en_core_web_sm\n","\n","token_anno = \"entities\" # note this may require changes for v3.0+ transformer models\n","negative_sample = True # turn on or off negative sampling"]},{"cell_type":"code","source":["TRAIN_DATA = []\n","x1, x2, x3, x4 = 0, 0, 0, 0\n","\n","print(\"Preparing training data...\")\n","\n","for index, row in tqdm(train_df.iterrows()):\n","    # get text of each sample test\n","    train_text = row['text']\n","    row_id = row['Id']\n","    label = row['cleaned_label']\n","    m = len(label)\n","    text = train_text.lower().split('|')\n","    #text = re.split('[?.,;\\n\\t&!()]+', train_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin matching\n","    for sentence in text:\n","        x1 += 1\n","        # clean text \n","        #sentence = clean_text(sentence)\n","        indexed = KMP(label, sentence)\n","        if indexed != []:\n","            n = len(sentence)\n","            elist = []\n","            for i in indexed:\n","                end = m + i\n","                a = ((end < n and sentence[end] == ' ') or end >= n) # can have NEGATIVE SAMPLING like \"ADNI-2\" (or are those negative ones?)\n","                b = ((i > 0 and sentence[i-1] == ' ') or i == 0)\n","                if a and b:\n","                    entity = (i, end, \"DATASET\") \n","                    elist.append(entity)\n","                    x3 += 1\n","            if elist == []: break # TEST (REMOVE IF NEEDED)\n","            x = (sentence, {token_anno:elist})\n","            TRAIN_DATA.append(x)\n","        \n","        elif negative_sample: \n","            x2 += 1\n","            if random.random() > 0.995:\n","                x4 += 1\n","                TRAIN_DATA.append((sentence, {token_anno:[]})) # TEST"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:01:10.976205Z","iopub.execute_input":"2021-06-02T09:01:10.976602Z","iopub.status.idle":"2021-06-02T09:15:56.212397Z","shell.execute_reply.started":"2021-06-02T09:01:10.976564Z","shell.execute_reply":"2021-06-02T09:15:56.206935Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["19661it [06:35, 49.69it/s]\n"]}]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]\nAmong 35961857 sentences,\n35908537 sentences have no title in them,\nmeaning 99.852% do not have title.\nSo we have 53320 sentences with titles.\nIn 53320 sentences, we obtained 53506 tokens (positive samples) that perfectly match titles.\nFor sentences without any title, we chose 358706 negative samples.\nThe ratio of positive vs negative samples is 14.92 : 100.\n"]}],"source":["print(\"[INFO]\")\n","print(\"Among %d sentences,\"%x1)\n","print(\"%d sentences have no title in them,\"%x2)\n","print(\"meaning %.3f%% do not have title.\"%(x2/x1*100))\n","print(\"So we have %d sentences with titles.\"%(x1-x2))\n","print(\"In %d sentences, we obtained %d tokens (positive samples) that perfectly match titles.\"%(x1-x2, x3))\n","print(\"For sentences without any title, we chose %d negative samples.\"%x4)\n","print(\"The ratio of positive vs negative samples is %.2f : 100.\"%(x3 / x4 * 100))\n"]},{"source":["### Configure training pipeline and train the spaCy model\n","\n","Download official model with `python3 -m spacy download en_core_web_sm`"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training configurations\n","output_dir = \"output/\"\n","n_iter = 15 # number of training iteration"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU: True\n","Created blank 'en' model\n"]}],"source":["print(\"Using GPU:\", spacy.prefer_gpu())\n","\n","# load the model\n","if model == \"specified\":\n","    nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","elif model == \"generic\":\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    print(\"Loaded generic model en_core_web_sm\")\n","elif model is not None:\n","    nlp = spacy.load(model)  \n","    print(\"Loaded model '%s'\" % model)\n","else:\n","    nlp = spacy.blank('en')  \n","    print(\"Created blank 'en' model\")\n","\n","# add ner component to pipeline\n","if 'ner' not in nlp.pipe_names:\n","    ner = nlp.create_pipe('ner') # version < 3.0\n","    nlp.add_pipe(ner, last=True) # verions < 3.0\n","    #nlp.add_pipe(\"ner\", last=True) # version >= 3.0 only\n","    #ner = nlp.get_pipe(\"ner\") # version >= 3.0 only\n","else:\n","    ner = nlp.get_pipe('ner')\n","\n","# add all labels to the ner\n","#for _, annotations in TRAIN_DATA:\n","#    for ent in annotations.get('entities'):\n","#        ner.add_label(ent[2])\n","ner.add_label(\"DATASET\") # only one category so no for loop\n","\n","# configure optimizer (may not work with spaCy v3 transformers)\n","optimizer = nlp.begin_training()\n","\n","# configure pipeline components to disable\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"]},{"cell_type":"code","source":["# start training\n","with nlp.disable_pipes(*other_pipes):  # only train NER\n","    for itn in range(n_iter):\n","        random.shuffle(TRAIN_DATA)\n","        losses = {}\n","        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001)) # HYPERPARAMETERS\n","        # version < 3.0 use this\n","        for batch in tqdm(batches):\n","            texts, annotations = zip(*batch)\n","            nlp.update(\n","                texts, # or [texts] if not using batch\n","                annotations, # or [annotations] if not using batch\n","                drop=0.5,  \n","                sgd=optimizer,\n","                losses=losses)\n","        \n","        # For spaCy v3.0+ use the similar for loops in archived.ipynb\n","            \n","        print(\"\\titeration %d, ner loss: %.2f\"%(itn+1, losses['ner']))"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T09:53:36.713695Z","iopub.execute_input":"2021-06-02T09:53:36.714046Z","iopub.status.idle":"2021-06-02T10:33:58.243031Z","shell.execute_reply.started":"2021-06-02T09:53:36.713992Z","shell.execute_reply":"2021-06-02T10:33:58.242166Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["14092it [08:32, 27.50it/s]\n","0it [00:00, ?it/s]\titeration 1, ner loss: 16403.95\n","14092it [08:27, 27.78it/s]\n","0it [00:00, ?it/s]\titeration 2, ner loss: 11636.79\n","14092it [08:20, 28.14it/s]\n","\titeration 3, ner loss: 10677.45\n","14092it [07:57, 29.51it/s]\n","0it [00:00, ?it/s]\titeration 4, ner loss: 10381.32\n","14092it [07:55, 29.65it/s]\n","0it [00:00, ?it/s]\titeration 5, ner loss: 10086.71\n","14092it [08:41, 27.02it/s]\n","\titeration 6, ner loss: 10257.75\n","14092it [09:10, 25.60it/s]\n","\titeration 7, ner loss: 10441.79\n","14092it [09:13, 25.48it/s]\n","\titeration 8, ner loss: 10276.88\n","14092it [09:02, 26.00it/s]\n","\titeration 9, ner loss: 10391.56\n","14092it [09:03, 25.94it/s]\n","\titeration 10, ner loss: 10218.74\n","14092it [09:03, 25.94it/s]\n","\titeration 11, ner loss: 10218.12\n","14092it [08:54, 26.38it/s]\n","\titeration 12, ner loss: 10324.05\n","14092it [08:45, 26.81it/s]\n","\titeration 13, ner loss: 10320.37\n","14092it [08:12, 28.62it/s]\n","0it [00:00, ?it/s]\titeration 14, ner loss: 10401.77\n","14092it [08:04, 29.06it/s]\n","0it [00:00, ?it/s]\titeration 15, ner loss: 10113.48\n","14092it [08:05, 29.03it/s]\n","0it [00:00, ?it/s]\titeration 16, ner loss: 10251.16\n","14092it [08:06, 28.99it/s]\n","0it [00:00, ?it/s]\titeration 17, ner loss: 10241.61\n","14092it [08:06, 28.99it/s]\n","0it [00:00, ?it/s]\titeration 18, ner loss: 10262.46\n","14092it [08:05, 29.04it/s]\n","0it [00:00, ?it/s]\titeration 19, ner loss: 10299.59\n","14092it [08:05, 29.01it/s]\n","0it [00:00, ?it/s]\titeration 20, ner loss: 10436.34\n","14092it [08:06, 28.96it/s]\n","0it [00:00, ?it/s]\titeration 21, ner loss: 10291.68\n","14092it [08:06, 28.98it/s]\n","0it [00:00, ?it/s]\titeration 22, ner loss: 10267.06\n","14092it [08:36, 27.30it/s]\n","\titeration 23, ner loss: 10309.68\n","14092it [08:43, 26.94it/s]\n","0it [00:00, ?it/s]\titeration 24, ner loss: 10520.79\n","14092it [08:16, 28.37it/s]\titeration 25, ner loss: 10552.53\n","\n"]}]},{"source":["## Testing and Exporting Model"],"cell_type":"markdown","metadata":{}},{"source":["# sample test\n","for text, _ in TRAIN_DATA:\n","    doc = nlp(text)\n","    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n","    break\n","\n","# save model\n","if output_dir is not None:\n","    output_dir = Path(output_dir)\n","    if not output_dir.exists():\n","        output_dir.mkdir()\n","    nlp.to_disk(output_dir)\n","    print(\"Saved model to\", output_dir)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:34:08.26658Z","iopub.execute_input":"2021-06-02T10:34:08.266973Z","iopub.status.idle":"2021-06-02T10:34:08.308941Z","shell.execute_reply.started":"2021-06-02T10:34:08.266937Z","shell.execute_reply":"2021-06-02T10:34:08.308088Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Entities []\n","Saved model to output\n"]}]},{"source":["## Inference with Trained Model\n","Generates the submission file"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def clean_text(txt):\n","    ''' DO NOT DELETE: Official function for submission text cleaning '''\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n","    #return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.968814Z","iopub.execute_input":"2021-06-02T09:01:10.969416Z","iopub.status.idle":"2021-06-02T09:01:10.974589Z","shell.execute_reply.started":"2021-06-02T09:01:10.969374Z","shell.execute_reply":"2021-06-02T09:01:10.973608Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["%%time\n","tqdm.pandas()\n","sample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_fp))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:01:10.928338Z","iopub.execute_input":"2021-06-02T09:01:10.929053Z","iopub.status.idle":"2021-06-02T09:01:10.967383Z","shell.execute_reply.started":"2021-06-02T09:01:10.929012Z","shell.execute_reply":"2021-06-02T09:01:10.966455Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/ry/.local/lib/python3.8/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n","100%|██████████| 4/4 [00:00<00:00, 511.69it/s]CPU times: user 5.96 ms, sys: 3.97 ms, total: 9.93 ms\n","Wall time: 11.8 ms\n","\n"]}]},{"cell_type":"code","source":["# STRING MATCHING BLOCK\n","temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n","temp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\n","temp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n","\n","existing_labels = set(temp_1 + temp_2 + temp_3)\n","id_list = []\n","lables_list = []\n","\n","# load model 'EN'\n","#nlp2 = spacy.load(output_dir) # loading an model can be slower?\n","\n","for index, row in tqdm(sample_sub.iterrows()):\n","    # get text of each sample test\n","    sample_text = row['text']\n","    row_id = row['Id']\n","    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n","\n","    cleaned_labels = temp_df['cleaned_label'].to_list()\n","\n","    texts = sample_text.lower().split('.')\n","    #text = re.split('[?.,;\\n\\t&!]', sample_text) # can't have sample_text.lower() since I need to find consecutive caps\n","\n","    # begin search\n","    # matching\n","    \n","    for known_label in existing_labels:   # for each label in the known set\n","        # EXACT MATCH\n","        if known_label in sample_text.lower():   # find the EXACT label in text \n","            cleaned_labels.append(clean_text(known_label)) # if found, then append to the list for further formatting\n","    \n","    # THIS METHOD BELOW IS MUCH FASTER!!!!!\n","    # SOURCE: https://spacy.io/usage/processing-pipelines\n","    # SOURCE 2: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n","    # THANK YOU SOOO MUCH!!!\n","    # Disabling pipeline components also helps\n","    for doc in nlp.pipe(texts, disable=other_pipes):\n","        cleaned_labels.extend([clean_text(entity.text) for entity in doc.ents if entity.label_ == \"DATASET\"])\n","    \n","\n","    #for sentence in texts:   \n","        #doc = nlp(text_cleaning(sentence))\n","    #    doc = nlp2(text_cleaning(sentence))\n","\n","    #    for entity in doc.ents:\n","    #        if entity.label_ == 'DATASET':\n","    #            cleaned_labels.append(clean_text(entity.text))   \n","            \n","        # CASE 1: FUZZYMATCH (see archived)\n","\n","        # CASE 2: Consecutive Capitalizations (see archived)\n","        \n","        \n","    #cleaned_labels = [clean_text(x) for x in cleaned_labels]\n","    cleaned_labels = set(cleaned_labels)\n","    lables_list.append('|'.join(cleaned_labels))\n","    id_list.append(row_id)"],"metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-02T10:39:26.643326Z","iopub.execute_input":"2021-06-02T10:39:26.643638Z","iopub.status.idle":"2021-06-02T10:39:53.6932Z","shell.execute_reply.started":"2021-06-02T10:39:26.64361Z","shell.execute_reply":"2021-06-02T10:39:53.691622Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["4it [00:00,  4.00it/s]\n"]}]},{"cell_type":"code","source":["submission = pd.DataFrame()\n","submission['Id'] = id_list\n","submission['PredictionString'] = lables_list"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.695152Z","iopub.execute_input":"2021-06-02T10:39:53.695494Z","iopub.status.idle":"2021-06-02T10:39:53.701857Z","shell.execute_reply.started":"2021-06-02T10:39:53.695457Z","shell.execute_reply":"2021-06-02T10:39:53.700815Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","submission.head()"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T10:39:53.703221Z","iopub.execute_input":"2021-06-02T10:39:53.703604Z","iopub.status.idle":"2021-06-02T10:39:53.720033Z","shell.execute_reply.started":"2021-06-02T10:39:53.703569Z","shell.execute_reply":"2021-06-02T10:39:53.719022Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     Id  \\\n","0  2100032a-7c33-4bff-97ef-690822c43466   \n","1  2f392438-e215-4169-bebf-21ac4ff253e1   \n","2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n","3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n","\n","                                    PredictionString  \n","0  s|adni|alzheimer s disease neuroimaging initia...  \n","1  school teacher salaries in|3 to|s|isco 88 inte...  \n","2  8 00|s|slosh meows|slosh moms|slosh|slosh mode...  \n","3         rural urban continuum codes|2008 see fig|s  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>s|adni|alzheimer s disease neuroimaging initia...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>school teacher salaries in|3 to|s|isco 88 inte...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>8 00|s|slosh meows|slosh moms|slosh|slosh mode...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>rural urban continuum codes|2008 see fig|s</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.721528Z","iopub.execute_input":"2021-06-02T10:39:53.721939Z","iopub.status.idle":"2021-06-02T10:39:53.729671Z","shell.execute_reply.started":"2021-06-02T10:39:53.721904Z","shell.execute_reply":"2021-06-02T10:39:53.728804Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["for item in submission[\"PredictionString\"]:\n","    print(item)\n","    print()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T10:39:53.731823Z","iopub.execute_input":"2021-06-02T10:39:53.732194Z","iopub.status.idle":"2021-06-02T10:39:53.741727Z","shell.execute_reply.started":"2021-06-02T10:39:53.732158Z","shell.execute_reply":"2021-06-02T10:39:53.740776Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["s|adni|alzheimer s disease neuroimaging initiative adni|47 10|alzheimer s disease neuroimaging initiative adni |8\n\nschool teacher salaries in|3 to|s|isco 88 international|trends in international mathematics and science study|nces common core of data|common core of data\n\n8 00|s|slosh meows|slosh moms|slosh|slosh model|coastal observation station|noaa storm surge inundation|3 00|sea lake and overland surges from hurricanes|noaa 2016 and|slosh and|coastal change science along\n\nrural urban continuum codes|2008 see fig|s\n\n"]}]}]}